{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Text Classification?\n",
        "\n",
        "Text classification is the process of assigning predefined categories or labels to textual data. It is a fundamental task in Natural Language Processing (NLP) with applications like:\n",
        "- **Spam detection**\n",
        "- **Sentiment analysis**\n",
        "- **Topic labeling**\n",
        "- **Language detection**\n",
        "- **Intent classification in chatbots**\n",
        "\n",
        "There are several approaches to perform text classification:\n",
        "\n",
        "1. **Traditional Machine Learning Models** (e.g., Naïve Bayes, Logistic Regression, Random Forest)\n",
        "2. **Deep Learning Models** (e.g., LSTMs)\n",
        "3. **Transformer-based Models** (e.g., BERT, DistilBERT, GPT)\n",
        "\n",
        "Each approach has its own strengths and is suited to different types of data and complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-04T15:46:44.064023Z",
          "iopub.status.busy": "2025-04-04T15:46:44.063607Z",
          "iopub.status.idle": "2025-04-04T15:46:44.070832Z",
          "shell.execute_reply": "2025-04-04T15:46:44.069599Z",
          "shell.execute_reply.started": "2025-04-04T15:46:44.063988Z"
        },
        "id": "BIyj7iBWgBRy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-04T15:46:44.072491Z",
          "iopub.status.busy": "2025-04-04T15:46:44.072157Z",
          "iopub.status.idle": "2025-04-04T15:46:46.465514Z",
          "shell.execute_reply": "2025-04-04T15:46:46.464498Z",
          "shell.execute_reply.started": "2025-04-04T15:46:44.072462Z"
        },
        "id": "sNMySHePgBR3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=['rec.sport.hockey', 'sci.space'], remove=('headers', 'footers', 'quotes'))\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioyEqXJtgBR3"
      },
      "source": [
        "The CountVectorizer is using the Bag-of-Words (BoW) model to convert text into a numerical format.\n",
        "\n",
        "How the Bag-of-Words model works:\n",
        "Tokenization – Splits text into individual words (tokens).\n",
        "\n",
        "Vocabulary Building – Creates a vocabulary of unique words from the training data.\n",
        "\n",
        "Word Count Representation – Converts each document into a numerical vector, where each dimension represents the count of a specific word in the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-04T15:46:46.467539Z",
          "iopub.status.busy": "2025-04-04T15:46:46.467257Z",
          "iopub.status.idle": "2025-04-04T15:46:46.800911Z",
          "shell.execute_reply": "2025-04-04T15:46:46.799992Z",
          "shell.execute_reply.started": "2025-04-04T15:46:46.467514Z"
        },
        "id": "7DjexWfrgBR5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# converting text to numerical using\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Naive Bayes\n",
        "Naive Bayes is a probabilistic classifier based on Bayes' Theorem, assuming feature independence. It is especially effective for high-dimensional problems and is commonly used for spam detection and text classification due to its simplicity and speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-04T15:46:46.802862Z",
          "iopub.status.busy": "2025-04-04T15:46:46.802482Z",
          "iopub.status.idle": "2025-04-04T15:46:46.812115Z",
          "shell.execute_reply": "2025-04-04T15:46:46.810964Z",
          "shell.execute_reply.started": "2025-04-04T15:46:46.802821Z"
        },
        "id": "WU6plD2kgBR6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Train Naive Bayes Model and find the y_pred\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vec, y_train)\n",
        "y_pred_nb = nb.predict(X_test_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression\n",
        "Logistic Regression is a linear model for binary classification. It estimates the probability that an instance belongs to a particular class using a logistic function. Despite its name, it is widely used for classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-04T15:46:46.813482Z",
          "iopub.status.busy": "2025-04-04T15:46:46.813122Z",
          "iopub.status.idle": "2025-04-04T15:46:46.869925Z",
          "shell.execute_reply": "2025-04-04T15:46:46.869076Z",
          "shell.execute_reply.started": "2025-04-04T15:46:46.813442Z"
        },
        "id": "tVMkZjYmgBR6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Train Logistic Regression Model\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_vec, y_train)\n",
        "y_pred_lr = lr.predict(X_test_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest\n",
        "Random Forest is an ensemble learning method that builds multiple decision trees and merges their outputs. It handles nonlinearities well, is robust to overfitting in many cases, and works well with both categorical and numerical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-04T15:46:46.871234Z",
          "iopub.status.busy": "2025-04-04T15:46:46.870868Z",
          "iopub.status.idle": "2025-04-04T15:46:47.613134Z",
          "shell.execute_reply": "2025-04-04T15:46:47.612156Z",
          "shell.execute_reply.started": "2025-04-04T15:46:46.8712Z"
        },
        "id": "hb3mAEvDgBR6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Train Random Forest Model\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(X_train_vec, y_train)\n",
        "y_pred_rf = rf.predict(X_test_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSTM\n",
        "LSTM (Long Short-Term Memory) networks are a type of Recurrent Neural Network (RNN) designed to model sequential data. LSTMs are particularly useful for capturing long-term dependencies in text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-04T15:46:47.614595Z",
          "iopub.status.busy": "2025-04-04T15:46:47.614245Z",
          "iopub.status.idle": "2025-04-04T15:47:03.887772Z",
          "shell.execute_reply": "2025-04-04T15:47:03.886931Z",
          "shell.execute_reply.started": "2025-04-04T15:46:47.614559Z"
        },
        "id": "LwwqJSxygBR7",
        "outputId": "b696c00a-6975-4d93-8bad-bc91468ef64d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - accuracy: 0.6078 - loss: 0.6764 - val_accuracy: 0.8719 - val_loss: 0.4053\n",
            "Epoch 2/3\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.8782 - loss: 0.3956 - val_accuracy: 0.8920 - val_loss: 0.2345\n",
            "Epoch 3/3\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.9406 - loss: 0.1894 - val_accuracy: 0.9070 - val_loss: 0.2147\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
          ]
        }
      ],
      "source": [
        "# LSTM Model\n",
        "max_words = 5000\n",
        "embedding_dim = 128\n",
        "max_length = 100\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "X_train_pad = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=max_length)\n",
        "X_test_pad = tf.keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=max_length)\n",
        "\n",
        "# Define LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_length),\n",
        "    SpatialDropout1D(0.2),\n",
        "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train LSTM\n",
        "lstm_model.fit(X_train_pad, y_train, epochs=3, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "y_pred_lstm = (lstm_model.predict(X_test_pad) > 0.5).astype(\"int32\").flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Models (BERT, GPT)\n",
        "Transformers are attention-based models that have revolutionized NLP. Pre-trained models like BERT, DistilBERT, and GPT understand deep semantic meaning and context. They are state-of-the-art for many NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-04T15:49:23.688035Z",
          "iopub.status.busy": "2025-04-04T15:49:23.687643Z",
          "iopub.status.idle": "2025-04-04T15:50:25.292443Z",
          "shell.execute_reply": "2025-04-04T15:50:25.291411Z",
          "shell.execute_reply.started": "2025-04-04T15:49:23.688005Z"
        },
        "id": "OUbbelFigBR9",
        "outputId": "809ede3a-ef76-4d6e-dae8-dcc34bc24935",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "# Transformer Model (BERT)\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "transformer_model = pipeline(\"text-classification\", model=\"distilbert-base-uncased\")\n",
        "\n",
        "X_test_truncated = [tokenizer_bert(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\") for text in X_test]\n",
        "y_pred_bert = [int(transformer_model(tokenizer_bert.decode(text['input_ids'][0], skip_special_tokens=True))[0]['label'][-1]) for text in X_test_truncated]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-04T15:51:11.765092Z",
          "iopub.status.busy": "2025-04-04T15:51:11.76469Z",
          "iopub.status.idle": "2025-04-04T15:51:11.819739Z",
          "shell.execute_reply": "2025-04-04T15:51:11.818637Z",
          "shell.execute_reply.started": "2025-04-04T15:51:11.765059Z"
        },
        "id": "KfE57imxgBR-",
        "outputId": "2f78b3af-bf9a-4432-e68c-2a5f1e6c3645",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naïve Bayes Performance:\n",
            "Accuracy: 0.949748743718593\n",
            "Confusion Matrix:\n",
            "[[192  10]\n",
            " [ 10 186]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95       202\n",
            "           1       0.95      0.95      0.95       196\n",
            "\n",
            "    accuracy                           0.95       398\n",
            "   macro avg       0.95      0.95      0.95       398\n",
            "weighted avg       0.95      0.95      0.95       398\n",
            "\n",
            "\n",
            "\n",
            "Logistic Regression Performance:\n",
            "Accuracy: 0.9447236180904522\n",
            "Confusion Matrix:\n",
            "[[185  17]\n",
            " [  5 191]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.92      0.94       202\n",
            "           1       0.92      0.97      0.95       196\n",
            "\n",
            "    accuracy                           0.94       398\n",
            "   macro avg       0.95      0.95      0.94       398\n",
            "weighted avg       0.95      0.94      0.94       398\n",
            "\n",
            "\n",
            "\n",
            "Random Forest Performance:\n",
            "Accuracy: 0.9472361809045227\n",
            "Confusion Matrix:\n",
            "[[183  19]\n",
            " [  2 194]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.91      0.95       202\n",
            "           1       0.91      0.99      0.95       196\n",
            "\n",
            "    accuracy                           0.95       398\n",
            "   macro avg       0.95      0.95      0.95       398\n",
            "weighted avg       0.95      0.95      0.95       398\n",
            "\n",
            "\n",
            "\n",
            "LSTM Performance:\n",
            "Accuracy: 0.907035175879397\n",
            "Confusion Matrix:\n",
            "[[187  15]\n",
            " [ 22 174]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91       202\n",
            "           1       0.92      0.89      0.90       196\n",
            "\n",
            "    accuracy                           0.91       398\n",
            "   macro avg       0.91      0.91      0.91       398\n",
            "weighted avg       0.91      0.91      0.91       398\n",
            "\n",
            "\n",
            "\n",
            "BERT Transformer Performance:\n",
            "Accuracy: 0.5050251256281407\n",
            "Confusion Matrix:\n",
            "[[194   8]\n",
            " [189   7]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.96      0.66       202\n",
            "           1       0.47      0.04      0.07       196\n",
            "\n",
            "    accuracy                           0.51       398\n",
            "   macro avg       0.49      0.50      0.36       398\n",
            "weighted avg       0.49      0.51      0.37       398\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Performance Metrics\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"{model_name} Performance:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    # print(\"F1-score:\", f1_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"\\n\")\n",
        "\n",
        "evaluate_model(y_test, y_pred_nb, \"Naive Bayes\")\n",
        "evaluate_model(y_test, y_pred_lr, \"Logistic Regression\")\n",
        "evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n",
        "evaluate_model(y_test, y_pred_lstm, \"LSTM\")\n",
        "evaluate_model(y_test, y_pred_bert, \"BERT Transformer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "- **Naive Bayes** is fast and works well for small and clean datasets. It assumes feature independence, which may limit performance on complex data.\n",
        "- **Logistic Regression** provides a strong and interpretable baseline for binary classification tasks.\n",
        "- **Random Forest** can model complex relationships and feature interactions but may be less efficient on large sparse data.\n",
        "- **LSTM** models context over time and is well-suited for longer sequences and more nuanced language.It means that LSTM models handle text that has finer details, subtle meanings, or context-dependent variations - like irony, sarcasm, or long dependencies : better than simpler models.\n",
        "- **Transformers (BERT/GPT)** are state-of-the-art for understanding text context and semantics. They require more resources but offer superior performance on diverse text classification tasks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "text_classification_comparison",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
