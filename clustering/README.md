# Clustering

Welcome to the **Clustering** section! This folder provides an introduction to clustering techniques, which are essential for unsupervised learning tasks. Clustering helps you group similar data points together, making it easier to identify patterns or categories within your data.

**Note**: The notebooks here are designed for beginners. They introduce foundational concepts but do not cover all available clustering methods or advanced techniques. For a more comprehensive understanding, please refer to the recommended resources provided below.

## üìÇ Structure

This folder currently includes:
- **Agglomerative Clustering**: A hierarchical clustering technique based on merging clusters.
- **DBSCAN**: A density-based clustering method, useful for identifying clusters of arbitrary shape.
- **K-Means**: A popular clustering method that partitions data into a specified number of clusters.

Each section includes **assignments** to help reinforce your understanding, along with **solutions** for self-assessment.

## üîó Learning Flow

Follow these steps to build a strong foundation in clustering techniques:

### 1. **Agglomerative Clustering**
   - **Purpose**: This hierarchical clustering method begins by treating each data point as an individual cluster, then successively merges the closest clusters.
   - **Topics to Cover**:
     - Basics of hierarchical clustering
     - Linkage criteria (single, complete, average)
     - Dendrograms for visualizing hierarchical clusters
   - **Resources**:
     - [Agglomerative Clustering (Sklearn Documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)
     - [Hierarchical Clustering Tutorial](https://www.datacamp.com/tutorial/hierarchical-clustering-python) (DataCamp)
     - [StatQuest's Hierarchical Clustering](https://www.youtube.com/watch?v=7xHsRkOdVwo) (video)

### 2. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**
   - **Purpose**: DBSCAN identifies clusters based on data density, making it useful for detecting clusters with arbitrary shapes.
   - **Topics to Cover**:
     - Core points, border points, and noise points
     - Selecting parameters like epsilon and minimum samples
     - DBSCAN‚Äôs advantages for handling noise and non-linear shapes
   - **Resources**:
     - [DBSCAN (Sklearn Documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)
     - [DBSCAN Tutorial](https://towardsdatascience.com/dbscan-clustering-algorithm-5e3c78e021eb) (Towards Data Science)
     - [StatQuest's DBSCAN](https://www.youtube.com/watch?v=RDZUdRsdvOU) (video)

### 3. **K-Means**
   - **Purpose**: K-Means is a partitioning clustering algorithm that aims to split data into a predefined number of clusters.
   - **Topics to Cover**:
     - Centroid calculation and cluster assignment
     - Elbow method for choosing the optimal number of clusters
     - Limitations of K-Means (e.g., sensitivity to initial centroids)
   - **Resources**:
     - [K-Means (Sklearn Documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
     - [K-Means Clustering Guide](https://www.kaggle.com/code/kandij/k-means-clustering-tutorial/notebook) (Kaggle tutorial)
     - [K means Clustering Algorithm](https://www.youtube.com/watch?v=EItlUEPCIzM) (video)

### 4. **K-Medoids**  
  - **Purpose**: K-Medoids is a partitioning clustering algorithm that aims to split data into a predefined number of clusters while being more 
      robust to outliers by using medoids instead of centroids.  
  - **Topics to Cover**:  
    - Medoid selection and cluster assignment  
    - PAM (Partitioning Around Medoids) algorithm  
    - Limitations of K-Medoids (e.g., higher computational cost compared to K-Means)  
  - **Resources**:  
    - [K-Medoids (Scikit-Learn Alternative)](https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html)  
    - [K-Medoids Clustering Guide](https://www.kaggle.com/code/farzanehkh/k-medoids-clustering) (Kaggle tutorial)  
    - [K-Medoids Clustering Algorithm](https://www.youtube.com/watch?v=2qzxYfuY5aM) (video)  

### 5. **Clustering Quality Evaluation**
   - **Purpose**: Clustering evaluation metrics help assess the performance of the Clustering Algorithms.
   - **Topics to Cover**:
     - Silhouette Score
     - Davies Bouldin Index
     - Adjusted Rand Score (ARI)
   - **Resources**:
     - [Clustering Performance Evaluation (sklearn documentation)](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)
     - [Clustering Metrics in Machine Learning (GeeksForGeeks)](https://www.geeksforgeeks.org/clustering-metrics/)

## üìù Assignments and Solutions

Each clustering method comes with assignments designed to help you apply the concepts you've learned. Solutions are provided for self-evaluation. Try to complete the assignments independently before checking the solutions for the best learning experience.

## üèÅ Getting Started

1. **Begin with Agglomerative Clustering**: Start by understanding how hierarchical clustering builds clusters step-by-step.
2. **Explore DBSCAN**: Learn how DBSCAN groups data based on density, making it robust for non-linear data.
3. **Try K-Means**: Experiment with partitioning data into clusters, focusing on selecting the optimal number of clusters.
4. **Evaluate performance**: Assess the performance of the above mentioned algorithms and find the most suitable one for your data.

Happy clustering! Developing these skills will enable you to analyze data and identify patterns effectively. For further learning, refer to the documentation and tutorials linked above.
